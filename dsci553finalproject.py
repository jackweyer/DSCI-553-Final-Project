# -*- coding: utf-8 -*-
"""DSCI553FinalProject.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1P8n40eBuzrfLkywaQR5CLa3ZnTtE8Mye
"""

# -*- coding: utf-8 -*-
"""Copy of Copy of dsci553project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1vf8s9nrJ6524N_OCLa-cfHqlu02dmb_G

# DATA
* use Spark RDD except for inputting to model
* yelp_train.csv: (user_id, business_id, stars)
* yelp_val.csv 
* review_train.json: review data only for the training pairs (user, business)
* user.json: all user metadata
* business.json: all business metadata, including locations, attributes, and categories

# TASK
* RMSE error
* sys.argv[1]: folder path 
* '/content/drive/MyDrive/dsci 553 project data/yelp_val.csv': test file
* sys.argv[3]: output file
* output: CSV, 
* comment(?) the: method description, error distribution, RMSE, execution time on val data
* full points if RMSE < 0.98
* <= 25 minutes
"""

!pip install pyspark
from pyspark import SparkContext, SparkConf
import os
import sys
import csv
#!pip install xgboost==0.72.1
import xgboost as xgb
import json
import pandas as pd
#pd.set_option('display.max_columns', 15)
from xgboost.sklearn import XGBRegressor
import math
from sklearn.metrics import mean_squared_error
from datetime import datetime, timedelta
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import make_scorer, mean_squared_error
import numpy as np

sc = SparkContext('local[*]', 'task2_1')
sc.setLogLevel('WARN')
os.environ['PYSPARK_PYTHON'] = '/usr/local/bin/python3.6'
os.environ['PYSPARK_DRIVER_PYTHON'] = '/usr/local/bin/python3.6'
#conf = SparkConf().setMaster("local") \
 #      .setAppName("final_project") \
  #     .set("spark.executor.memory", "4g") \
   #    .set("spark.driver.memory", "4g")
#sc = SparkContext(conf=conf)

import time

start_time = time.time()

"""# Pre-process"""

# Training data without header
train = sc.textFile('/content/drive/MyDrive/dsci 553 project data/yelp_train.csv')
header = train.first()
train = train.filter(lambda line: line != header) \
.map(lambda line: line.split(",")) #u,b,stars

# Val data without header
val = sc.textFile('/content/drive/MyDrive/dsci 553 project data/yelp_val.csv')
header = val.first()
val = val.filter(lambda line: line != header) \
.map(lambda line: line.split(","))

# User data
user = sc.textFile('/content/drive/MyDrive/dsci 553 project data/user.json') \
.map(lambda row: json.loads(row)) \
.map(lambda x: (x["user_id"], (x["review_count"], int(x["yelping_since"][:4]), x["average_stars"], 0 if x["friends"] == 'None' else len(x["friends"].split(',')), x["useful"], x["funny"], x["cool"], x["fans"], x["compliment_hot"],x["compliment_more"],x["compliment_profile"],x["compliment_cute"],x["compliment_list"],x["compliment_note"],x["compliment_plain"],x["compliment_cool"],x["compliment_funny"],x["compliment_writer"],x["compliment_photos"]))).persist()

# Business data.. attributes next?
business = sc.textFile('/content/drive/MyDrive/dsci 553 project data/business.json'). \
map(lambda row: json.loads(row)) \
.map(lambda x: (x["business_id"], (x["latitude"], x["longitude"], x["stars"], x["review_count"], x["is_open"], x["categories"], x["hours"]))).persist()
# is_open is 0 or 1

# training data with user and business
train_w_users = train.map(lambda x: (x[0], (x[1], x[2]))).join(user) # join with user

train_w_users_clean = train_w_users.map(lambda x: (x[1][0][0], (x[0], x[1][0][1], x[1][1][0], x[1][1][1], x[1][1][2], x[1][1][3], x[1][1][4], x[1][1][5], x[1][1][6], x[1][1][7], x[1][1][8], x[1][1][9], x[1][1][10], x[1][1][11], x[1][1][12], x[1][1][13], x[1][1][14], x[1][1][15], x[1][1][16], x[1][1][17], x[1][1][18]))) # business as key

all_data = train_w_users_clean.join(business).map(lambda x: x[:1] + x[1][0] + x[1][1]) #join with business

train_collect = all_data.collect() #into memory
#as data frame
train_df = pd.DataFrame(train_collect, columns = ['business_id', 'user_id', 'target', "user_review_count", "yelping_since", "user_stars", "friends", "useful", "funny", "cool", "fans", "compliment_hot","compliment_more","compliment_profile","compliment_cute","compliment_list","compliment_note","compliment_plain","compliment_cool","compliment_funny","compliment_writer","compliment_photos","latitude", "longitude", "business_stars", "business_review_count", "is_open", "categories", "hours"])

"""## Top 20 categories"""

# get category counts
cat_counts = business_categories = sc.textFile('/content/drive/MyDrive/dsci 553 project data/business.json'). \
map(lambda row: json.loads(row)) \
.map(lambda x: (x["business_id"], x["categories"])).persist() \
.filter(lambda x: x[1] is not None) \
.flatMap(lambda x: x[1].split(',')) \
.map(lambda x: x.strip()) \
.map(lambda x: (x,1)) \
.reduceByKey(lambda x, y: x + y)

# filter to top 20 categories
top20 = cat_counts.sortBy(lambda x: x[1], ascending=False) \
.filter(lambda x: x[1] >= 6000)

# put into memory
top_20 = top20.map(lambda x: x[0]).collect()

# dictionary of business with top categories
business_top_cats = business.map(lambda x: (x[0], x[1][5])) \
.filter(lambda x: x[1] != None) \
.map(lambda x: (x[0], [cat.strip() for cat in x[1].split(',') if x[1]])) \
.flatMapValues(lambda x: x) \
.map(lambda x: (x[1], x[0])) \
.filter(lambda x: x[0] in top_20) \
.map(lambda x: (x[1], x[0])) \
.groupByKey() \
.mapValues(list) \
.collectAsMap()

# prepare dataframe
for ohe_col in top_20:
  train_df[top_20] = 0

# populate dataframe with OHE for top categories
for index, row in train_df.iterrows():
    business_id = row['business_id']

    if business_id in business_top_cats:

        for key in business_top_cats[business_id]:
            train_df.at[index, key] = 1

"""## Business hours"""

business_hours = sc.textFile('/content/drive/MyDrive/dsci 553 project data/business.json'). \
map(lambda row: json.loads(row)) \
.map(lambda x: (x["business_id"], x["hours"])).collectAsMap()

hm = pd.DataFrame(business_hours).T.rename_axis("business_id")

def set_business_hours(day):
  hm[day+'-open'] = hm[day].apply(lambda x: str(x).split('-')[0] if str(x).lower() not in ['none', 'nan'] else 0)
  hm[day+'-close'] = hm[day].apply(lambda x: str(x).split('-')[1] if str(x).lower() not in ['none', 'nan'] else 0)
  hm[day+'-open'] = hm[day+'-open'].apply(lambda x: datetime.strptime(x, "%H:%M") if x not in [0] else 0)
  hm[day+'-close'] = hm[day+'-close'].apply(lambda x: datetime.strptime(x, "%H:%M") if x not in [0] else 0)
  hm[day+'-open'] = hm[day+'-open'].apply(lambda x: x.hour + x.minute / 60 if x not in [0] else 0)
  hm[day+'-close'] = hm[day+'-close'].apply(lambda x: x.hour + x.minute / 60 if x not in [0] else 0)
  hm[day+'-hours_open'] = hm.apply(lambda x: x[day+'-close'] - x[day+'-open'] if (x[day+'-close'] >= x[day+'-open']) else x[day+'-close'] + 24 - x[day+'-open'], axis=1)

days = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']

for day in days:
  set_business_hours(day) 

hm = hm.reset_index()
hm = hm.drop(['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', "Sunday"], axis=1)
hm.index = pd.to_numeric(hm.index)
train_df = train_df.merge(hm, on = 'business_id')
# set up train objects
Xtrain = train_df.drop(['user_id', 'business_id', 'target', 'categories', 'hours'], axis=1)
ytrain = train_df['target']

"""TIPS DATA"""

train_df

tips = sc.textFile('/content/drive/MyDrive/dsci 553 project data/tip.json'). \
map(lambda row: json.loads(row)) \
.map(lambda x: (x["business_id"], x["text"]))

import string

def remove_punctuation_and_lower(text):
    return text.translate(str.maketrans("", "", string.punctuation)).lower()

# get most common words
#tips.flatMap(lambda x: x[1].split(' ')).map(lambda x: (remove_punctuation_and_lower(x), 1)).reduceByKey(lambda x,y:x+y).sortBy(lambda x: x[1], ascending = False).take(200)
# great, good, service, best, love, amazing, awesome, dont, nice, friendly, like, no, staff, delicious, really, free, new, happy, excellent, wait

common_words = ['great', 'good', 'service', 'best', 'love', 'amazing', 'awesome', 'dont', 'nice', 'friendly', 'like', 'no', 'staff', 'delicious', 'really', 'free', 'new', 'happy', 'excellent', 'wait']

business_common_words = tips.map(lambda x: (x[0], x[1].split(' '))).flatMapValues(lambda x:x).filter(lambda x: x[1] in common_words)

bus_word_count = business_common_words.map(lambda x: ((x[0], x[1]),1)).reduceByKey(lambda x,y:x+y).map(lambda x: (x[0][0], (x[0][1], x[1])))

# Create the DataFrame from the dictionary
#df = pd.DataFrame(columns = ['business_id'] + common_words)

#businesses = bus_word_count.map(lambda x: x[0]).collect()
#df['business_id'] = businesses

data_dict = {}
for key, value in bus_word_count.collect():
    if key not in data_dict:
        data_dict[key] = {}
    data_dict[key][value[0]] = value[1]

business_word_df_raw = pd.DataFrame.from_dict(data_dict, orient='index').fillna(0).astype(int)

business_word_df_raw = business_word_df_raw.reset_index().rename(columns = {'index': 'business_id'})

train_df = pd.merge(train_df, business_word_df_raw, on = "business_id", how = "left").fillna(0)

"""Attributes"""

att_dict = sc.textFile('/content/drive/MyDrive/dsci 553 project data/business.json'). \
map(lambda row: json.loads(row)) \
.map(lambda x: (x["business_id"], x["attributes"])).filter(lambda x: x[1] is not None).collectAsMap() # some none values

#T/f
t_f_attributes = ['RestaurantsGoodForGroups', 'RestaurantsReservations', 'RestaurantsTakeOut', 'Caters', 'BusinessAcceptsBitcoin', 'Open24Hours', 'OutdoorSeating', 'RestaurantsDelivery', 'DriveThru', 'BYOB', 'RestaurantsCounterService', 'BusinessAcceptsCreditCards', 'GoodForKids', 'HasTV', 'DogsAllowed', 'RestaurantsTableService', 'AcceptsInsurance', 'WheelchairAccessible', 'CoatCheck', 'Corkage', 'GoodForDancing', 'HappyHour', 'BikeParking', 'ByAppointmentOnly']
uh = pd.DataFrame(att_dict).T.rename_axis("business_id")
binary_attributes = uh[t_f_attributes].applymap(lambda x: 1 if x == 'True' else 0 if x == 'False' else 0.5) # true 1 false 0 nodata 0.5

train_df = train_df.merge(binary_attributes, on = 'business_id', how = "left").fillna(0.5)

# categorical
categorical_attributes = ['BYOBCorkage', 'Smoking', 'RestaurantsAttire', 'AgesAllowed', 'NoiseLevel', 'Alcohol', 'WiFi']
for cat_att in categorical_attributes:
  train_df = train_df.merge(pd.get_dummies(uh[cat_att], prefix=cat_att), on = "business_id", how = "left").fillna(0)

uh['RestaurantsPriceRange2'] = uh['RestaurantsPriceRange2'].fillna(0).astype(int)
train_df = train_df.merge(uh['RestaurantsPriceRange2'], on = "business_id", how = "left").fillna(2)

import ast
# further split
nested = ['Music', 'BusinessParking', 'GoodForMeal', 'BestNights', 'HairSpecializesIn']
uh_nested = uh[nested].reset_index()
for nest_var in nested:
  uh[nest_var].fillna("{}", inplace=True)
  ohe_vars = pd.get_dummies(pd.json_normalize(uh[nest_var].apply(eval)), columns=pd.json_normalize(uh[nest_var].apply(eval)).columns)
  ohe_vars = ohe_vars.filter(like = "_True")
  uh_nested = pd.concat([uh_nested, ohe_vars],axis=1)

OHE_vars = uh_nested.drop(['Music', 'BusinessParking', 'GoodForMeal', 'BestNights', 'HairSpecializesIn'], axis=1)

train_df = train_df.merge(OHE_vars, on = "business_id", how = "left").fillna(0)

"""VAL DATA"""

# create dictionary for testing data
user_dict = user.collectAsMap()
bus_dict = business.collectAsMap()

# fill Test data to match the training and impute for cold-start
val_filled = val.map(lambda x: (x[1], x[0], x[2], user_dict.get(x[0], 0)[0], user_dict.get(x[0], 2018)[1], user_dict.get(x[0], 3.7)[2], user_dict.get(x[0], 0)[3], user_dict.get(x[0], 0)[4], user_dict.get(x[0], 0)[5], user_dict.get(x[0], 0)[6], user_dict.get(x[0], 0)[7], user_dict.get(x[0], 0)[8], user_dict.get(x[0], 0)[9], user_dict.get(x[0], 0)[10], user_dict.get(x[0], 0)[11], user_dict.get(x[0], 0)[12], user_dict.get(x[0], 0)[13], user_dict.get(x[0], 0)[14], user_dict.get(x[0], 0)[15], user_dict.get(x[0], 0)[16], user_dict.get(x[0], 0)[17], user_dict.get(x[0], 0)[18], bus_dict.get(x[1], 38.9)[0], bus_dict.get(x[1], -97.7)[1], bus_dict.get(x[1], 3.7)[2], bus_dict.get(x[1], 0)[3], bus_dict.get(x[1], 0.5)[4], bus_dict.get(x[1], '')[5], bus_dict.get(x[1], '')[6]))
val_collect = val_filled.collect()
val_df = pd.DataFrame(val_collect, columns = ['business_id', 'user_id', 'target', "user_review_count", "yelping_since", "user_stars", "friends", "useful", "funny", "cool", "fans", "compliment_hot","compliment_more","compliment_profile","compliment_cute","compliment_list","compliment_note","compliment_plain","compliment_cool","compliment_funny","compliment_writer","compliment_photos","latitude", "longitude", "business_stars", "business_review_count", "is_open", "categories", "hours"])

for ohe_col in top_20:
  val_df[top_20] = 0
for index, row in val_df.iterrows():
    business_id = row['business_id']
    # Check if the user_id is present in the dictionary
    if business_id in business_top_cats:
        # Update the corresponding column with 1
        for key in business_top_cats[business_id]:
            val_df.at[index, key] = 1

val_df = val_df.merge(hm, on = 'business_id')

val_df = pd.merge(val_df, business_word_df_raw, on = "business_id", how = "left").fillna(0)

val_df = val_df.merge(binary_attributes, on = 'business_id', how = "left").fillna(0.5)

for cat_att in categorical_attributes:
  val_df = val_df.merge(pd.get_dummies(uh[cat_att], prefix=cat_att), on = "business_id", how = "left").fillna(0)

val_df = val_df.merge(uh['RestaurantsPriceRange2'], on = "business_id", how = "left").fillna(2)

val_df = val_df.merge(OHE_vars, on = "business_id", how = "left").fillna(0)

"""SET UP OBJECTS"""

Xtrain = train_df.drop(['user_id', 'business_id', 'target', 'hours', 'categories'], axis = 1)

X_test = val_df.drop(['user_id', 'business_id', 'target', 'hours', 'categories'], axis = 1)

y_test = val_df["target"]

ytrain = train_df['target']

scoring_metric = make_scorer(lambda y_true, y_pred: -np.sqrt(mean_squared_error(y_true, y_pred)), greater_is_better=True)
xgb = XGBRegressor()
parameters = {
              'objective':['reg:linear'],
              'n_estimators': [10000],
              'learning_rate': [.05, .02], #lower values next. center on .05
              'max_depth': [6],
              'early_stopping_rounds': [60],
              'min_child_weight': [5],
              'subsample': [1], #NEED TO INCLUDE 1
              'colsample_bytree': [.1, .3],
              'alpha': [0] #NEED TO INCLUDE 0
              }

xgb_grid = GridSearchCV(xgb,
                        parameters,
                        cv = 3,
                        n_jobs = 1,
                        verbose=10,
                        scoring = scoring_metric)

"""# Pre-process test data"""


#X_test = val_df.drop(['user_id', 'business_id', 'target', 'categories', 'hours'], axis=1)
            

"""# MODEL"""

#scoring_metric = make_scorer(lambda y_true, y_pred: -np.sqrt(mean_squared_error(y_true, y_pred)), greater_is_better=True)
#xgb = XGBRegressor()
#parameters = {
#              'objective':['reg:linear'],
#              'n_estimators': [2000],
#              'learning_rate': [.05, .025], #lower values next. center on .05
#              'max_depth': [6, 4],
#              'colsample_bytree': [.55, .7],
#              'early_stopping_rounds': [20],
#              'min_child_weight': [1, 5],
#              'subsample': [1, .7], #NEED TO INCLUDE 1
#              'colsample_bytree': [0.7, .55],
#              'alpha': [0, .25] #NEED TO INCLUDE 0
#              }
#
#xgb_grid = GridSearchCV(xgb,
#                        parameters,
#                        cv = 3,
#                        n_jobs = 1,
#                        verbose=10,
#                        scoring = scoring_metric)

#y_test = val_df["target"]

#xgb_grid.fit(Xtrain,
#         ytrain,
#        eval_set = [(X_test, y_test)])

model = XGBRegressor(objective='reg:linear', eval_metric='rmse', n_estimators = 1000, learning_rate = .05, max_depth = 6, colsample_bytree = .55, random_state=553) #reg:squarederror

model.fit(Xtrain,
         ytrain,
          verbose = True)
modelpreds = model.predict(X_test)

np.sqrt(mean_squared_error(modelpreds, y_test))

output = pd.DataFrame()
output['user_id'] = val_df['user_id']
output['business_id'] = val_df['business_id']
output['prediction'] = modelpreds

output.to_csv('final_preds.csv', index=False)